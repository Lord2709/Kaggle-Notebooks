{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "591/591 [==============================] - 21s 32ms/step - loss: 0.2347 - accuracy: 0.9283 - val_loss: 0.0809 - val_accuracy: 0.9731\n",
      "Epoch 2/10\n",
      "591/591 [==============================] - 20s 35ms/step - loss: 0.0643 - accuracy: 0.9799 - val_loss: 0.0601 - val_accuracy: 0.9807\n",
      "Epoch 3/10\n",
      "591/591 [==============================] - 27s 46ms/step - loss: 0.0438 - accuracy: 0.9862 - val_loss: 0.0424 - val_accuracy: 0.9862\n",
      "Epoch 4/10\n",
      "591/591 [==============================] - 24s 41ms/step - loss: 0.0339 - accuracy: 0.9893 - val_loss: 0.0385 - val_accuracy: 0.9876\n",
      "Epoch 5/10\n",
      "591/591 [==============================] - 24s 41ms/step - loss: 0.0267 - accuracy: 0.9913 - val_loss: 0.0414 - val_accuracy: 0.9867\n",
      "Epoch 6/10\n",
      "591/591 [==============================] - 21s 35ms/step - loss: 0.0220 - accuracy: 0.9931 - val_loss: 0.0372 - val_accuracy: 0.9900\n",
      "Epoch 7/10\n",
      "591/591 [==============================] - 28s 47ms/step - loss: 0.0200 - accuracy: 0.9938 - val_loss: 0.0376 - val_accuracy: 0.9881\n",
      "Epoch 8/10\n",
      "591/591 [==============================] - 27s 45ms/step - loss: 0.0168 - accuracy: 0.9944 - val_loss: 0.0417 - val_accuracy: 0.9864\n",
      "Epoch 9/10\n",
      "591/591 [==============================] - 26s 44ms/step - loss: 0.0123 - accuracy: 0.9960 - val_loss: 0.0454 - val_accuracy: 0.9876\n",
      "Epoch 10/10\n",
      "591/591 [==============================] - 26s 44ms/step - loss: 0.0119 - accuracy: 0.9962 - val_loss: 0.0373 - val_accuracy: 0.9905\n",
      "132/132 [==============================] - 1s 10ms/step - loss: 0.0373 - accuracy: 0.9905\n",
      "Validation accuracy: 0.9904761910438538\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import Sequential, layers\n",
    "\n",
    "# Load the training and test datasets\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# Separate the labels (target variable) and pixel values from the training dataset. \n",
    "# Normalize pixel values by dividing by 255 to scale them between 0 and 1.\n",
    "X_train = train_data.drop(columns=['label']).values.astype('float32') / 255.0\n",
    "y_train = train_data['label'].values\n",
    "X_test = test_data.values.astype('float32') / 255.0\n",
    "\n",
    "# Reshape the pixel values into a 28x28 matrix with a single channel (for grayscale images) to be compatible with the CNN architecture.\n",
    "# Specifying -1 in the first dimension of the reshape operation allows NumPy to automatically determine the size of that dimension based \n",
    "# on the total number of elements in the array and the sizes of the other dimensions.\n",
    "\n",
    "X_train = X_train.reshape((-1, 28, 28, 1))\n",
    "X_test = X_test.reshape((-1, 28, 28, 1))\n",
    "\n",
    "# Split the training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "# Define the model architecture\n",
    "model = Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_val, y_val))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(X_val, y_val)\n",
    "print('Validation accuracy:', test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Adam optimizer is a popular choice for training neural network models, including convolutional neural networks (CNNs), due to its several advantages:\n",
    "\n",
    "1. **Adaptive Learning Rate**: Adam automatically adapts the learning rate during training based on the gradients of the parameters. It maintains separate learning rates for each parameter and adjusts them individually, which helps improve convergence and training speed.\n",
    "\n",
    "2. **Momentum Optimization**: Adam incorporates the concept of momentum, which helps accelerate the optimization process by accumulating gradients over time. This momentum term allows the optimizer to navigate through flat regions and escape local minima more effectively.\n",
    "\n",
    "3. **Efficient Updates**: Adam efficiently updates the parameters using a combination of adaptive learning rates and momentum. It keeps track of the first and second moments of the gradients to compute the adaptive learning rates, resulting in more stable and efficient updates.\n",
    "\n",
    "4. **Robustness to Noisy Gradients**: Adam is robust to noisy or sparse gradients, making it suitable for a wide range of optimization problems, including deep learning tasks with large datasets and complex architectures.\n",
    "\n",
    "5. **Ease of Use**: Adam is easy to use and typically requires minimal tuning of hyperparameters compared to other optimization algorithms. It has become a popular choice for many deep learning practitioners due to its good performance across various tasks and datasets.\n",
    "\n",
    "Overall, the adaptive learning rate, momentum optimization, and robustness to noisy gradients make Adam a suitable optimizer for training neural networks, including CNNs, and it is often the default choice for many deep learning applications. However, it's always a good practice to experiment with different optimizers and tune hyperparameters based on the specific requirements of your model and dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------\n",
    "-----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice of batch size in neural network training is a hyperparameter that can significantly impact the training process and the performance of the model. The batch size determines the number of samples that are propagated through the network before the model's parameters are updated based on the computed gradients.\n",
    "\n",
    "Here are some considerations for choosing a batch size of 64 in the context of training the model for the Digit Recognizer competition:\n",
    "\n",
    "1. **Computational Efficiency**: Larger batch sizes can lead to more efficient computations, especially on hardware accelerators like GPUs. With a batch size of 64, the computations can be parallelized effectively, leading to faster training times compared to smaller batch sizes.\n",
    "\n",
    "2. **Stochasticity**: Using smaller batch sizes introduces more randomness into the training process, which can help the model generalize better to unseen data. However, excessively small batch sizes may lead to noisy updates and slower convergence. A batch size of 64 strikes a balance between computational efficiency and stochasticity.\n",
    "\n",
    "3. **Memory Constraints**: Larger batch sizes require more memory to store intermediate activations and gradients during backpropagation. A batch size of 64 is typically manageable for most modern hardware configurations and doesn't require excessive memory resources.\n",
    "\n",
    "4. **Smoothness of Gradient Estimates**: Larger batch sizes provide smoother gradient estimates, which can result in more stable training dynamics and faster convergence towards the optimal solution. However, too large of a batch size may lead to suboptimal solutions or hinder the generalization ability of the model.\n",
    "\n",
    "In summary, the choice of a batch size of 64 is a common practice in deep learning experiments due to its balance between computational efficiency, stochasticity, and memory constraints. However, the optimal batch size may vary depending on the specific characteristics of the dataset, model architecture, and hardware resources available for training. It's often recommended to experiment with different batch sizes and monitor the model's performance to determine the most suitable value for a given task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "875/875 [==============================] - 8s 9ms/step\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test set\n",
    "predictions = np.argmax(model.predict(X_test), axis=-1)\n",
    "\n",
    "# Prepare submission file\n",
    "submission = pd.DataFrame({'ImageId': range(1, len(predictions) + 1), 'Label': predictions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
